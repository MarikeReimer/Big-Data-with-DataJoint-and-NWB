{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2:\n",
    "## This chapter demonstrates how to work with data that is NWB format and in the DataJoint pipeline.\n",
    "\n",
    "The first part of the chapter walks through retrieving data from the file created in Chapter 1.  The second part walks through retreiving that data from the DataJoint pipelines.\n",
    "\n",
    "Important: Before continuing, update ImportsAndTableDefinitions.py with your DataJoint credentials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting marikelreimer@tutorial-db.datajoint.io:3306\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pynwb #NWB API\n",
    "import pandas as pd\n",
    "import h5py #Provides methods to supplement NWB\n",
    "import datajoint as dj #\n",
    "import csv \n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from pynwb import NWBHDF5IO\n",
    "from nwbwidgets import nwb2widget #Displays file contents in Jupyter\n",
    "from matplotlib import pylab as plt #Needed for ERD diagrams\n",
    "\n",
    "#Import our classes and functions\n",
    "from ImportsAndTableDefinitions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata setup\n",
    "\n",
    "So that we can access the table definitions, they are saved in a python script called ImportsAndTableDefinitions.py residing in the root directory. This script should als be updated with your DataJoint credentials, so that you can connect to DataJoint's tutorial server. To promote standardization across a large group of people, it is advisable to share these definitions in Git/Github. Whenever code needs to be updated, having one source of record, greatly streamlines updates, especially for multiple users. This also allows the use of Git/Github's pull method to update your local definitions.\n",
    "\n",
    "As in Chapter 1, we store variables in a Jupyter notebook.  It is better practice to store data in files, which will be covered in Chapters 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update these variables as needed:\n",
    "subject_id = 'Mouse_5025'\n",
    "session_id = 'Session_22'\n",
    "experiment_path = 'C:/Users/meowm/OneDrive/DataWarehouse/Experimenter1' #Update with your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These variables don't need to be updated\n",
    "\n",
    "#Construct path to data directory\n",
    "data_directory = experiment_path + '/' + subject_id + \"/\" + session_id + \"/\"\n",
    "\n",
    "#Uniqely identify NWB file\n",
    "identifier = subject_id + \"_\" + session_id #This uniquely identifies a file\n",
    "\n",
    "#Create a uniqe name ending in '.nwb'\n",
    "nwbfile_name = identifier + '.nwb'\n",
    "\n",
    "#Navigate to data directory\n",
    "os.chdir(data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Reading data from NWB files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an NWB file, we make use of the NWBHDF5IO class using 'r' to indicate that we are read-mode.  Unlike the example in the previous chapter we did not use the 'with' method.  This means that data from the file will be available in all cells of the notebook, however and we must enter a command to close it when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the file\n",
    "io = NWBHDF5IO(nwbfile_name, mode='r')\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NWB API lets us retrieve data from our subject table (from the 'General' group) like so:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject pynwb.file.Subject at 0x1932907376328\n",
       "Fields:\n",
       "  date_of_birth: 2017-04-03 11:00:00-04:00\n",
       "  description: Fuzzy\n",
       "  genotype: B6\n",
       "  sex: F\n",
       "  species: Mus musculus\n",
       "  subject_id: Mouse_5025\n",
       "  weight: 25g"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject = nwbfile.subject\n",
    "subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations are substantially streamlined compared to the equivalent operations done with visit() and visititems() from the Hdf5 library.  \n",
    "\n",
    "The '.' used to specify a path will be familiar to Matlab users, as it is the method used to access values stored in structured arrays (structs) which are synonomous with the group organization in hdf5. Matlab files are a type of hdf5 file, which is why the syntax is the same.\n",
    "\n",
    "As before, we can retrieve the subject ID like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mouse_5025'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject.subject_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries data works similarly, with the caveat that we must know the name of the module and timeseries before we can retrieve data.  We begin by displaying the modules in the processing group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'behavior': behavior pynwb.base.ProcessingModule at 0x1932907376584\n",
       " Fields:\n",
       "   data_interfaces: {\n",
       "     Session_22 <class 'pynwb.behavior.SpatialSeries'>\n",
       "   }\n",
       "   description: Spatial series containing coordinates representating the location of a mouse}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the modules in the Processing group\n",
    "nwbfile.processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the name of the module, we use the list() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behavior\n"
     ]
    }
   ],
   "source": [
    "module_name = list(nwbfile.processing)\n",
    "\n",
    "#Remove brackets from module name\n",
    "module_name = module_name[0]\n",
    "print(module_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module name gives us access to its data interfaces, which allows us to retreive the name of our timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Session_22': Session_22 pynwb.behavior.SpatialSeries at 0x1932907376072\n",
      "Fields:\n",
      "  comments: Will need to correct camera jitter\n",
      "  conversion: 1.0\n",
      "  data: <HDF5 dataset \"data\": shape (17768, 2), type \"<f8\">\n",
      "  description: The position of a mouse in an arena is transformed into X,Y coordinates\n",
      "  rate: 30.0\n",
      "  reference_frame: Zero refers to the bottom left corner of the rig, when viewed from above\n",
      "  resolution: -1.0\n",
      "  starting_time: 0.0\n",
      "  starting_time_unit: seconds\n",
      "  unit: meters\n",
      "}\n",
      "Session_22\n"
     ]
    }
   ],
   "source": [
    "module = nwbfile.processing[module_name]\n",
    "\n",
    "#This displays a detailed description of the data interfaces in the processing module\n",
    "print(module.data_interfaces)\n",
    "\n",
    "#This retrieves the name of the timeseries from the processing module\n",
    "timeseries_name = list(module.data_interfaces)\n",
    "\n",
    "#Remove brackets from timeseries name\n",
    "timeseries_name = timeseries_name[0]\n",
    "print(timeseries_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the containers are used to create a path to the data.  To reach our data we must access the Processing group, then the behavior module, then the timeseries, and ultimately the contents of the data stored in it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 90.  72.]\n",
      " [ 91.  67.]\n",
      " [ 89.  70.]\n",
      " ...\n",
      " [316. 192.]\n",
      " [314. 194.]\n",
      " [312. 195.]]\n"
     ]
    }
   ],
   "source": [
    "#Retrieve coordinate data\n",
    "timeseries_data = nwbfile.processing[module_name][timeseries_name].data[:]\n",
    "print(timeseries_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done working with the file so we close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part  2 Working with Data from DataJoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by instantiating the tables we defined in Chapter 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we can use them, the tables from chapter 1 must be instantiated:\n",
    "mouse = Mouse()\n",
    "session = Session()\n",
    "position = Position()\n",
    "positionStatistics = PositionStatistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint provides a function that retrieves datasets as dictionaries, a python structure which stores key value pairs.  This creates a dictionary of data in the Mouse table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subject_id': 'Mouse_5025',\n",
       "  'date_of_birth': datetime.datetime(2017, 4, 3, 11, 0),\n",
       "  'genotype': 'B6',\n",
       "  'sex': 'F',\n",
       "  'species': 'Mus musculus',\n",
       "  'subject_description': 'Fuzzy',\n",
       "  'weight': '25g'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data = mouse.fetch(as_dict=True)\n",
    "our_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve a complete dataset, we must use the join operator * to connect our tables and store them in a variable called dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = mouse * session * position * positionStatistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint's fetch() method returns each dataset as a dictionary, so this command can be used to programmatically access data from single or multiple sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subject_id': 'Mouse_5025',\n",
       "  'name': 'Session_22',\n",
       "  'date_of_birth': datetime.datetime(2017, 4, 3, 11, 0),\n",
       "  'genotype': 'B6',\n",
       "  'sex': 'F',\n",
       "  'species': 'Mus musculus',\n",
       "  'subject_description': 'Fuzzy',\n",
       "  'weight': '25g',\n",
       "  'comments': 'Will need to correct camera jitter',\n",
       "  'description': 'The position of a mouse in an arena is transformed into X,Y coordinates',\n",
       "  'experiment_description': 'Demonstrate creating a single NWB file and loading a DataJoint pipeline from an experimental session',\n",
       "  'experimenter': 'Experimenter 1',\n",
       "  'identifier': 'Mouse_5025_Session_22',\n",
       "  'institution': 'Yale University',\n",
       "  'lab': 'Tan lab',\n",
       "  'rate': 30.0,\n",
       "  'raw_data_link': 'C:/Users/meowm/OneDrive/DataWarehouse/Experimenter1/Mouse_5025/Session_22/raw_video.avi',\n",
       "  'reference_frame': 'Zero refers to the bottom left corner of the rig, when viewed from above',\n",
       "  'session_description': 'Determine if mouse has a place preference',\n",
       "  'session_start_time': datetime.datetime(2020, 7, 7, 15, 36, 13),\n",
       "  'starting_time': 0.0,\n",
       "  'coordinates': array([[ 90.,  72.],\n",
       "         [ 91.,  67.],\n",
       "         [ 89.,  70.],\n",
       "         ...,\n",
       "         [316., 192.],\n",
       "         [314., 194.],\n",
       "         [312., 195.]]),\n",
       "  'left_side': 9856.0,\n",
       "  'right_side': 7912.0,\n",
       "  'preference_index': 0.10941}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_data = dataset.fetch(as_dict=True)\n",
    "our_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next chapter, our use case addresses creating NWB files and a DataJoint dataset from existing data.  It \n",
    "demonstrates bulk loading of data from diverse raw data sources in a standard directory structure.  \n",
    "\n",
    "The row of data in our pipeline will cause an error because it we would create a duplicate entry in the database.  To address this, we clear the pipeline with mouse.drop()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`marikelreimer_tutorial`.`mouse` (1 tuples)\n",
      "`marikelreimer_tutorial`.`session` (1 tuples)\n",
      "`marikelreimer_tutorial`.`_position` (1 tuples)\n",
      "`marikelreimer_tutorial`.`__position_statistics` (1 tuples)\n",
      "Proceed? [yes, No]: yes\n",
      "Tables dropped.  Restart kernel.\n"
     ]
    }
   ],
   "source": [
    "mouse.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that DataJoint removed all dependent tables, preventing the possiblity of orphaned data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
